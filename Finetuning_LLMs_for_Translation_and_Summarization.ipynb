{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "46jcqT-avDxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I am going to explore finetuning a pre-trained LLM from huggingface for the purpoes of translating Russian to English. I will also use a different LLM to then summarize it. I will be using the pytorch library."
      ],
      "metadata": {
        "id": "Mo5A0zq5vHR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Libraries"
      ],
      "metadata": {
        "id": "VLFaNBf4vZN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch\n",
        "\n",
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments, pipeline, BartForConditionalGeneration, BartTokenizer\n",
        "from datasets import load_dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "KKhMYH20v3kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset and Model for Finetuning"
      ],
      "metadata": {
        "id": "DrXCnT-Gv4vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am using the Helsinki-NLP/opus-mt-ru-en model for translation and the opus-100 dataset with English and Russian sentence pairs."
      ],
      "metadata": {
        "id": "eb8PnHzswtLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ru\")\n",
        "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "zERIxWhXwFBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning the LLM"
      ],
      "metadata": {
        "id": "R2L3PfOOwGEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing the Data"
      ],
      "metadata": {
        "id": "b4IPpHV_wiY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will begin  by setting the inputs and targets appropriately. Russian being the input and English being the target. Then it is time to properly tokenize the sentence pairs and pad the text for uniformity.\n"
      ],
      "metadata": {
        "id": "FEA06Bo5w5XI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [example['ru'] for example in examples['translation']]\n",
        "    targets = [example['en'] for example in examples['translation']]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "mMU1N3LUwmtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spliting the Data for Testing and Training"
      ],
      "metadata": {
        "id": "auI5VutxxZDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "validation_dataset = tokenized_datasets[\"validation\"]"
      ],
      "metadata": {
        "id": "LcmznytzxeaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the Parameters and Training the Model"
      ],
      "metadata": {
        "id": "W679jgtZxiZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I set the training arguments to best fit the GPU used and the dataset type. Experiment with adjusting the argumetns for different results."
      ],
      "metadata": {
        "id": "Dg_0nAbWzki0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    gradient_accumulation_steps=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    fp16=True,\n",
        "    fp16_opt_level=\"O2\",\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=1000,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    run_name=\"opus_mt_ru_en_translation\",\n",
        "    disable_tqdm=False,\n",
        "    dataloader_num_workers=8,\n",
        "    seed=42,\n",
        "    gradient_checkpointing=True,\n",
        "    dataloader_pin_memory=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "WKgQ9HIQxrHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation and Summarization"
      ],
      "metadata": {
        "id": "vxLlDPFdxuRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping the Text"
      ],
      "metadata": {
        "id": "e8kRxdF-x2aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the text is webscraped from the Russian language wikipedia article on machine learning. Then it is pasted and words counted."
      ],
      "metadata": {
        "id": "HBT1Npjex6RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_website_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    paragraphs = soup.find_all('p')\n",
        "    website_text = \" \".join([para.get_text() for para in paragraphs])\n",
        "    return website_text\n",
        "\n",
        "url = \"https://ru.wikipedia.org/wiki/%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5\"\n",
        "\n",
        "website_text = scrape_website_text(url)\n",
        "\n",
        "print(\"Word Count:\")\n",
        "print(len(website_text.split()))\n",
        "print(\"Scraped Text:\")\n",
        "print(website_text)"
      ],
      "metadata": {
        "id": "TOgZO7evx_aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translating"
      ],
      "metadata": {
        "id": "BykI4E3yySqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is used to translate the scraped text."
      ],
      "metadata": {
        "id": "jvE8M4LCzJXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    translated = model.generate(**inputs)\n",
        "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "    return translated_text\n",
        "\n",
        "translated_text = translate_text(website_text)\n",
        "\n",
        "print(\"Word Count:\")\n",
        "print(len(website_text.split()))\n",
        "print(\"\\nTranslated Text:\")\n",
        "print(\"\\n\".join(translated_text.split(\". \")))"
      ],
      "metadata": {
        "id": "-Z1SOkSlyWAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarizing"
      ],
      "metadata": {
        "id": "2CrAoNr5yuwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the translated text is then summarized using the bart-large-cnn model"
      ],
      "metadata": {
        "id": "qOAz8CMFzbBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(translated_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "summary_ids = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    num_beams=4,\n",
        "    max_length=100,\n",
        "    early_stopping=True,\n",
        "    no_repeat_ngram_size=2,\n",
        "    length_penalty=1.0\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Summary Word Count:\")\n",
        "print(len(summary.split()))\n",
        "print(\"\\nSummarized Text Text:\")\n",
        "print(\"\\n\".join(summary.split(\". \")))"
      ],
      "metadata": {
        "id": "F8A4N5r0y5wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "Nkh7MbG3y81O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you for reading. Any feedback is welcome."
      ],
      "metadata": {
        "id": "gpMMPqNSzB12"
      }
    }
  ]
}